{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Simulated Data\n",
    "\n",
    "This notebook contains instructions on generating your own synthetic data to test estimator performance under a variety of controlled conditions.  The simulated datasets are created with the `make_classification_labels` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from statswag.datasets import make_classification_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set number of samples and labelers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_instances = 1000\n",
    "num_labelers = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the ``make_classification_labels`` function, we need to input a set of ground truth labels (``y``), the number of labelers desired (``n_labelers``), and a confusion matrix for each labeler (``confusion``).  For the ``confusion`` parameter, you can provide either a single confusion matrix, that will be used for each labeler, or you can provide a (potentially) different matrix for each labeler.\n",
    "\n",
    "In the example below, we will build one confusion matrix for the first labeler and copy the same confusion matrix for all other labelers. This is to represent the scenario where there is one outlying good labeler amongst several poor performing labelers.\n",
    "\n",
    "You can use this function to generate labels representative of your own scenarios of interest.\n",
    "\n",
    "_Note: In the simulated data, all labelers are conditionally independent.  This may be updated in future versions._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2]\n",
      " [0]\n",
      " [1]\n",
      " [2]\n",
      " [0]]\n",
      "\n",
      "[[2 2 0 2 2 0]\n",
      " [0 2 0 0 0 2]\n",
      " [1 0 2 1 1 2]\n",
      " [2 2 2 2 2 1]\n",
      " [0 0 0 2 2 2]]\n"
     ]
    }
   ],
   "source": [
    "# Generate random ground truth labels (0, 1, or 2)\n",
    "y = np.random.randint(0, 3, num_instances)\n",
    "\n",
    "# Create a confusion matrix for the first labeler, here we use 3 classes\n",
    "diagonal_first = 0.9\n",
    "off_diagonal_first = (1.0-diagonal_first)/2.0\n",
    "confusion_mat_first = [[diagonal_first,off_diagonal_first,off_diagonal_first],\n",
    "                       [off_diagonal_first,diagonal_first,off_diagonal_first],\n",
    "                       [off_diagonal_first,off_diagonal_first,diagonal_first]]\n",
    "\n",
    "# Create a confusion matrix for the other labelers\n",
    "diagonal_other = 0.55\n",
    "off_diagonal_other = (1.0-diagonal_other)/2.0\n",
    "confusion_mat_other = [[diagonal_other,off_diagonal_other,off_diagonal_other],\n",
    "                       [off_diagonal_other,diagonal_other,off_diagonal_other],\n",
    "                       [off_diagonal_other,off_diagonal_other,diagonal_other]]\n",
    "\n",
    "# From the ground truth labels, produce the predicted labels\n",
    "labels = make_classification_labels(y=y,\n",
    "                                    n_labelers=num_labelers,\n",
    "                                    confusion=[confusion_mat_first]+\n",
    "                                    [confusion_mat_other for i in range(num_labelers-1)])\n",
    "\n",
    "# Display first 5 true and predicted labels\n",
    "print(np.expand_dims(y[0:5],axis=1))\n",
    "print('')\n",
    "print(labels[0:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the true labeler accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.915, 0.512, 0.563, 0.556, 0.579, 0.545])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from statswag.metrics import nan_accuracy\n",
    "true_labeler_accuracy = np.asarray([nan_accuracy(y,labels[:,col]) for col in range(np.size(labels,axis=1))])\n",
    "\n",
    "true_labeler_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run estimators on this dataset\n",
    "\n",
    "Note that Majority Vote produces worse estimates of labeler accuracy (the errors are considerably larger), particularly for the first (outlying) labeler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Errors\n",
      "[0.232 0.055 0.074 0.057 0.082 0.058]\n",
      "Mean Absolute Error\n",
      "0.09300000000000001\n"
     ]
    }
   ],
   "source": [
    "# Estimate the accuracies using the MV estimator\n",
    "from statswag.estimators import MajorityVote\n",
    "\n",
    "mv = MajorityVote()\n",
    "results = mv.fit(labels)\n",
    "print('Errors')\n",
    "print(true_labeler_accuracy - results['accuracies'])\n",
    "print('Mean Absolute Error')\n",
    "print(np.mean(np.abs(true_labeler_accuracy - results['accuracies'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Errors\n",
      "[-0.038 -0.013 -0.014 -0.01  -0.008 -0.014]\n",
      "Mean Absolute Error\n",
      "0.016166666666666645\n"
     ]
    }
   ],
   "source": [
    "# Estimate the accuracies using the IWMV estimator\n",
    "from statswag.estimators import IWMV\n",
    "\n",
    "iwmv = IWMV()\n",
    "results = iwmv.fit(labels)\n",
    "print('Errors')\n",
    "print(true_labeler_accuracy - results['accuracies'])\n",
    "print('Mean Absolute Error')\n",
    "print(np.mean(np.abs(true_labeler_accuracy - results['accuracies'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Errors\n",
      "[-0.06545671  0.013402    0.016388    0.01128057  0.02237434  0.01188404]\n",
      "Mean Absolute Error\n",
      "0.02346427672216254\n"
     ]
    }
   ],
   "source": [
    "# Estimate the accuracies using the Spectral estimator\n",
    "from statswag.estimators import Spectral\n",
    "\n",
    "spectral = Spectral()\n",
    "results = spectral.fit(labels)\n",
    "print('Errors')\n",
    "print(true_labeler_accuracy - results['accuracies'])\n",
    "print('Mean Absolute Error')\n",
    "print(np.mean(np.abs(true_labeler_accuracy - results['accuracies'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Errors\n",
      "[-0.06357893 -0.01760571 -0.0569633  -0.08961293 -0.04720874 -0.08206775]\n",
      "Mean Absolute Error\n",
      "0.059506225311042105\n"
     ]
    }
   ],
   "source": [
    "# Estimate the accuracies using the Agreement estimator\n",
    "from statswag.estimators import Agreement\n",
    "\n",
    "agreement = Agreement()\n",
    "results = agreement.fit(labels)\n",
    "print('Errors')\n",
    "print(true_labeler_accuracy - results['accuracies'])\n",
    "print('Mean Absolute Error')\n",
    "print(np.mean(np.abs(true_labeler_accuracy - results['accuracies'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Errors\n",
      "[-0.03930892  0.00889757 -0.00353322  0.00949756  0.00673285  0.00724001]\n",
      "Mean Absolute Error\n",
      "0.012535018639368475\n"
     ]
    }
   ],
   "source": [
    "from statswag.estimators import MLEOneParameterPerLabeler\n",
    "\n",
    "MLE = MLEOneParameterPerLabeler()\n",
    "ll_list = []\n",
    "results_list = []\n",
    "for i in range(5):\n",
    "    results_list.append(MLE.fit(labels))\n",
    "    ll_list.append(MLE.expert_models.log_likelihood(labels,results_list[i]['class_names']))\n",
    "index = np.argmax(ll_list)\n",
    "print('Errors')\n",
    "print(true_labeler_accuracy-results_list[index]['accuracies'])\n",
    "print('Mean Absolute Error')\n",
    "print(np.mean(np.abs(true_labeler_accuracy-results_list[index]['accuracies'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
